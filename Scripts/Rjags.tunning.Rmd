---
title: "Rjags parameters"
output: html_notebook
editor_options: 
  chunk_output_type: console
---
Adjusting rjags parameters to see how they affect the prob of presence of a particular ASV in the dataset

The jags model feeds from two main sources: 

  * The presence / absence matrix
  
  * The initial parameters for the true positive rate and the false positive rate
  
We can modify the matrix by subsetting the data and doing the detection of an ASV in a month or Site - and later keep all hashes that were true (prob of Occ > 0.8).

```{r}
library(tidyverse)
library(vegan)
library(rjags)
library(proxy)
library(here)
library(unmarked)
library(jagsUI)

```

## Custom functions
```{r}
model.round.nested <- function(list.hashes,n){
  require(dplyr)
  tibble.out <- paste("model", n , sep=".")
  tibble.in <- "data"
  tibble.in <- rlang::ensym(tibble.in)
  list.hashes <- mutate (list.hashes, !!tibble.out := map (!!tibble.in,  ~jags_for_presence(.x)))
}

ProbOcc <- function(x, psi, p11, p10, K){
 (psi*(p11^x)*(1-p11)^(K-x)) / ((psi*(p11^x)*(1-p11)^(K-x))+(((1-psi)*(p10^x))*((1-p10)^(K-x))))
}
sink("RoyleLink_prior.txt")
										cat("model {
										    # Priors
										     psi ~ dunif(0,1)
										    p11 ~ dunif(0.01,1)
										    p10 ~ dbeta(1,50)
										    
										    # Likelihood 
										    for (i in 1:S){
										    z[i] ~ dbern(psi)
										    p[i] <- z[i]*p11 + (1-z[i])*p10
										    for (j in 1:K){
										    Y[i,j] ~ dbern(p[i])
										    }
										    }
										    } ",fill=TRUE)
										sink()
										

																				
jags_for_presence <- function(.x){
  .x %>% 
    transmute( model =  map_dbl (data,
                             function(.y, doprint=FALSE, ni=3000,nt=2,nc=1,nb=1000,myparallel=TRUE){
                               
                               .y %>% ungroup %>% dplyr::select(-biol) -> .y # Reduce the tibble to just the presence/abs matrix
                               
                               jags.inits <- function()(list(psi=runif(1,0.05,0.95),p11=runif(1, 0.01,1),p10=rbeta(1,1,50))) # generates three random starting estimates of psi, p11 and p10
                               jags.data <- list (Y= .y,
                                                  S = nrow(.y),
                                                  K = ncol(.y)) 
                               jags.params <- c("psi","p11","p10")
                               model<-jags(data = jags.data, inits = jags.inits, parameters.to.save= jags.params, 
                                           model.file= "RoyleLink_prior.txt", n.thin= nt, n.chains= nc, 
                                           n.iter= ni, n.burnin = nb, parallel=myparallel)
                               
                               psihat <- model$summary["psi","50%"]
                               p11hat <- model$summary["p11","50%"]
                               p10hat <- model$summary["p10","50%"]    
                               modelSummaries <- model$summary
                               
                               nObs   <- max(rowSums(.y))
                               K <- ncol(.y)

                               
                               model.output <- ProbOcc(nObs, psihat, p11hat, p10hat, K) 

                               
										           return(model.output)
   
                               
                             }))
  
}
```


### Change false positive rate 

The Time it takes to calculate the fate for each Hash is enormous. I had to run it several times until I realized that running an Occupancy model for each Hash in a matrix of 261 samples x 3 PCR replicates will take ~ 8 days. To get to that conclusion I run several tests which I have stored in a different Rmarkdown ("test.Occupancy.Rmd").

My conclusions from that exercise are:

  * It is better to run all Miseq runs at once
  
  * What matters is the pattern of replication, ie in how many biological replicates are there 3 presences in 3 technical replicates, how many 2 presences and how many only once. So we can summarise the pattern of presence of each Hash with a A.B.C pattern, which is how many times apears once, twice and three times. And we can run the model only once per replication pattern
  
  * When there are few presences in the dataset, the output of the model is extremely variable
  
So we will start by creating two objects, one is the nested dataframe that has the 261*3 matrices, and the other one is the reduced information with the pattern of replication
```{r}
# Load the dataset
ASV.nested <- read_rds(here("Input","Cleaning.before.Occ.model"))
# Split by site
ASV.nested %>% 
  ungroup() %>% 
  select(Step2.tibble) %>% 
  unnest(Step2.tibble) %>%
  ungroup() %>% 
  separate(sample, into = c("biol","rep"), sep = "\\.") %>%
  separate(biol, into = c("Site", "biol")) %>% 
  group_by(Site) %>% 
  nest() -> nested.by.site

# For each Hash - calculate the pattern of replication

nested.by.site %>% 
  mutate(data = map(data,  function(.y){
        .y %>% 
          mutate (nReads = 1) %>% 
  spread(key = "rep", value = "nReads", fill= 0,drop = F) %>% 
      mutate (ndetections = `1`+`2`+`3`) %>% 
      group_by(ndetections, Hash) %>% 
      summarise(tot = sum(!is.na(ndetections)))

      })) -> nested.by.site
# And finally calculate the pattern of presence
nested.by.site %>% 
  unnest() %>% 
  spread(ndetections, tot,fill = 0) %>% 
  unite(repetition.level,`0`,`1`,`2`,`3`, sep = ".") -> Pattern.of.presence 

# Choose one representative
Pattern.of.presence %>% 
  group_by(repetition.level) %>%
  summarise(first = head(Hash,1),
            Site =  head(Site,1)) %>%
  unite(Site,first,col = "key", sep = ".", remove = F)-> unique.by.site

# Subset 
 ASV.nested %>% 
   ungroup() %>% 
   select(Step2.tibble) %>% 
  unnest(Step2.tibble) %>% 
  ungroup() %>% 
  separate(sample, into = c("biol","rep"), sep = "\\.") %>%
  separate(biol, into = c("Site", "biol")) %>% 
  group_by(Site) %>% 
  nest() %>% 
  mutate(data = map(data,
                    function(.y){
                      .y %>% 
                        mutate (nReads = 1) %>% 
                        spread(key = "rep", value = "nReads", fill= 0,drop = F)
  })) %>% 
  unnest() %>% 
  unite(Site, Hash, col = "key", sep = ".", remove = F) %>% 
# Select one  
  filter(key %in% unique.by.site$key) %>% 
  select(-key) %>% group_by(Site, Hash) %>% 
  nest %>% 
  group_by(Site) %>% 
  nest -> like.this 
```
Finally - run it ten times
```{r}
like.this %>% 
  mutate(Hashes = map (data , ~select(.x, Hash))) -> like.this
# Run 10 times
for (i in 1:10){
  like.this <- model.round.nested(like.this, n = i)
}
like.this %>% mutate(model.10chain = map(data, jags_for_presence)) -> all.combos.10chain

all.combos.10chain %>% write_rds(here("Input", "10chain.rds"))

all.combos.10chain <- read_rds(here("Input", "10chain.rds"))

all.combos.10chain %>% unnest(cols = c(Hashes, starts_with("model")), names_repair = "unique") -> row.df

row.df %>% 
  left_join(unique.by.site, by = c("Hash" = "first", "Site")) %>% 
  ungroup() %>% 
  select(-Hash, -Site) %>% 
  right_join(Pattern.of.presence) %>% 
  group_by(Hash) %>% tally(name = "max", sort = T)

row.df %>% ungroup() %>% rowwise(Site, Hash) %>% summarise(mean = mean(c_across(where(is.numeric))))
```

Now the first step is to compare these values with the other two treatments in which we pooled all sites together

```{r}

all.combos.10chain %>% 
  unnest (Hashes, model.10chain ) %>% 
 # select(Site, Hash, contains("chain")) %>% 
 # filter(Hash %in% test.for.occ.modeling.p10.beta.50$Hash) %>% 
#  gather(contains("model"), key = "Attempt", value = "Output") %>% 
  ggplot(aes(x = Hash, y = model, color = Site))+
  geom_boxplot(position = "dodge",
               outlier.color = "red") + 
  theme(axis.text.x = element_blank(), 
        axis.ticks.x = element_blank()) +
  geom_hline(yintercept = 0.8, color = "red") +
  facet_wrap(~Site, ncol = 1)

```


That looks nice - so this is the process we will follow: Analyze the Prob of Ocurrence for each Hash and Site - and if a Hash has a value in a Site greater than 0.8, then we will keep it.
```{r}
like.this %>% 
  unnest %>% 
  select(Site, Hash, contains("model")) %>% 
  write_csv("Occ.all.hashes.by.site.csv")

all.combos.10chain %>% 
  unnest (Hashes, model.10chain ) %>% 
  write_csv("Occ.all.hashes.10chain.csv")

```


Now let's rejoin the Hash.site combo with the original dataset, and filter out those hashes that has a 25% quantile lower 0.8

```{r}

#Hash.Site.combos <- read_csv("Occ.all.hashes.by.site.csv")
Hash.Site.combos <- read_csv("Occ.all.hashes.10chain.csv")

Hash.Site.combos %>%
  unite(Site,Hash, sep = ".", col = "key") %>% 
#  gather(contains("model"), key = "Attempt", value = "Output") %>% 
#  group_by(key) %>%
 # nest %>%
#  mutate(Q25 = map_dbl(data, 
 #                                   function(.x){
#                                      quantile(.x$Output, probs = 0.25)
 #                                   })) %>%
 # unnest(Q25) %>% #leftjoin this with the pattern of repetition
  left_join(Pattern.of.presence %>%
              unite(Site, Hash, sep = ".", col = "key") %>%
              select(key, repetition.level)) %>% 
  select(repetition.level, model) %>% # and now right join this with all Hashes
  right_join(Pattern.of.presence, by = "repetition.level") %>% 
  group_by (Hash) %>% 
  summarise (real = max (model, na.rm = T)) -> Occ.model.fate



Occ.model.fate %>% 
  arrange((real))

Occ.model.fate %>% write_csv("Occ.fate.csv")



```


## Without Site into the mix
```{r}
ASV.nested %>% 
    ungroup() %>% 
  select(Step2.tibble) %>% 
  unnest(Step2.tibble) %>%

  separate(sample, into = c("biol","rep"), sep = "\\.") %>%
  mutate(nReads = 1) %>% 
  spread(key = "rep", value = "nReads", fill= 0,drop = F) %>% 
  group_by(Hash) %>%
  mutate(ndetections = `1`+`2`+`3`) %>% 
   group_by(ndetections, Hash) %>%
  summarise(tot = sum(!is.na(ndetections))) %>% 
  spread(ndetections, tot,fill = 0) %>% 
  unite(repetition.level,`1`,`2`,`3`, sep = ".") -> Pattern.of.presence2
```


```{r}
Pattern.of.presence2 %>% 
  group_by(repetition.level) %>% 
  select(-`0`) %>% 
  summarise(first = head(Hash, 1)) -> key.fromrep.to.hash
key.fromrep.to.hash %>% 
  pull(first) -> Hashes.to.run

```




```{r}
ASV.nested %>% 
    ungroup() %>% 
  select(Step2.tibble) %>% 
  unnest(Step2.tibble) %>%

  separate(sample, into = c("biol","rep"), sep = "\\.") %>%
  mutate(nReads = 1) %>% 
  spread(key = "rep", value = "nReads", fill= 0,drop = F) %>% 
  filter (Hash %in% Hashes.to.run) %>% 
  nest(-Hash) -> Nested.Hashes

Nested.Hashes %>% 
  mutate (susp = "Yes") %>% 
  nest(-susp) -> Nested.Hashes

for (i in 1:10){
  Nested.Hashes <- model.round.nested(Nested.Hashes, n = i)
}

write_rds(Nested.Hashes, here("Input", "Occupancy.nosite.rds"))

all.hashes <- read_rds(here("Input", "Occupancy.nosite.rds"))
```


```{r}
all.hashes %>% unnest(cols = c(data, starts_with("model")), names_repair = "unique") -> row1.df

row1.df %>% ungroup() %>% rowwise( Hash) %>% summarise(mean = mean(c_across(where(is.numeric))), 
                                                       median =median(c_across(where(is.numeric))),
                                                       max  = max(c_across(where(is.numeric)))) %>%
  left_join(Pattern.of.presence2, by = "Hash") %>% # this adds the output to the key
  ungroup %>% select(-Hash) %>% 
  left_join(Pattern.of.presence2) %>% write_csv(here("Input", "Occ.fate.2020.csv")) # this adds the output to the Hashes
```

